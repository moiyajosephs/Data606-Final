---
title: "Josephs-Final"
author: "Moiya Josephs"
date: "2022-10-18"
output: pdf_document
---

## Data Preparation

```{r setup, echo=TRUE, results='hide', warning=FALSE, message=FALSE}
# load data
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(ggplot2)
library(curl)
library(psych)
library(car)
```

## Abstract


## Research question 

Does having a social life predict final grade? 


## Cases 

**What are the cases, and how many are there?**

There are 649 cases that represent students and their achievements in secondary education in two Portuguese schools.

## Data collection 

**Describe the method of data collection.**

The data was collected using school reports and questionnaires.

## Type of study 

**What type of study is this (observational/experiment)?**

This is observational study.

## Data Source 

**If you collected the data, state self-collected. If not, provide a citation/link.**

The data was collected by University of Minho and the dataset can be found here: 
Source: https://archive.ics.uci.edu/ml/datasets/Student+Performance


### Dependent Variable

**What is the response variable? Is it quantitative or qualitative?**

The response variable is the final grade of each student. It is quantitative.

### Independent Variable(s)

The independent variable is the variables that describe the students social life. To answer this questions the variables I classify as describing a students social life is their activities, romantic, family relationship, free time, going out and alcohol consumption. They are all qualitative.



## Relevant summary statistics 


```{r message=FALSE}
student_mat_csv <- "https://raw.githubusercontent.com/moiyajosephs/Data606-Final/main/student-mat.csv"
student_mat <- read_delim(curl(student_mat_csv),delim = ";")
```


```{r}
summary(student_mat)
```


```{r}
ggplot(student_mat, aes(sex)) + geom_bar()
```

```{r}
ggplot(student_mat, aes(G3)) + geom_bar() + facet_wrap(vars(sex))
```

Normally distributed with one obvious outlier. 



## The Romance Model

## Romantic

Students reply either yes or no if they are in a romantic relationship, indicated by the `romantic` variable.

```{r}

ggplot(student_mat, aes(romantic)) + geom_bar()
```


```{r}
romantic.model <- lm(G3 ~ romantic, data = student_mat)
summary(romantic.model)
```


```{r}
ggplot(student_mat, aes(x=romantic, y = G3)) +
  geom_boxplot()
```

```{r}
ggplot(student_mat, aes(x= G3)) +
  geom_histogram(bins = 10) +
  facet_wrap(vars(romantic))
```


Romantic is statistically significant according to the anova test. 

```{r}
summary_stats <- student_mat %>%
  group_by(romantic) %>%
  summarise(mean_by_group = mean(G3))
summary_stats
```


```{r}
describeBy(student_mat$G3, 
           group = student_mat$romantic, mat=TRUE)
```

```{r}
one.way <- aov(G3 ~ romantic, data = student_mat)
summary(one.way)
```

### Analyzing the r^2


```{r}
ggplot(data = romantic.model, aes(x = .fitted, y = .resid)) +
  geom_point() + 
  geom_jitter()+
  geom_hline(yintercept = 0, linetype = "dashed") +
  xlab("Fitted values") +
  ylab("Residuals")

```

## Do anova and explain the variability


## The Activities Model
Students reply yes or no if they have extra curricular activities, indicated by the `activities` variables.


```{r}
ggplot(student_mat, aes(x= G3)) +
  geom_histogram(bins = 10) +
  facet_wrap(vars(activities))
```


```{r}
activities.model <- lm(G3 ~ activities, data = student_mat)
summary(activities.model)
```
By looking at the summary of the `activities.model` we can see that activites estimate is not statistically significant as indicated by p-value which is greater than 0.05. As well as the F-statistic and p value given, which is also greater than 0.05.


```{r}

ggplot(data = activities.model, aes(x = .fitted, y = .resid)) +
  geom_point() +
  geom_jitter() +
  geom_hline(yintercept = 0, linetype = "dashed") +
  xlab("Fitted values") +
  ylab("Residuals")


```



## Freetime 

Freetime after school rated from 1 very low to 5 very high.

```{r}
ggplot(student_mat, aes(freetime)) + geom_bar()

```
```{r}
ggplot(student_mat, aes(x= G3)) +
  geom_histogram(bins = 10) +
  facet_wrap(vars(activities))
```



```{r}
freetime.model <- lm(G3 ~ freetime, data = student_mat)
summary(freetime.model)
```
Not significant!



## Family Relationship

`famrel` indicated the quality of family relationships on a scale from (numeric: from 1 - very bad to 5 - excellent)


```{r}
ggplot(student_mat, aes(famrel)) + geom_bar()

```

```{r}
famrel.model <- lm(G3 ~ famrel, data = student_mat)
summary(famrel.model)
```

No significance

## Going Out

`goout` indicates if students go out with friends rated from (numeric: from 1 - very low to 5 - very high)


```{r}
ggplot(student_mat, aes(goout)) + geom_bar()

```


```{r}
goout.model <- lm(G3 ~ goout, data = student_mat)
summary(goout.model)

```

Significance!!

## Do anova and explain the variability



## Alcohol Consumption

Dalc indicated the workday alcohol consumption from very low to very high and Walc indicates weekend alcohol consumption, also from very low to very high.


```{r}
ggplot(student_mat, aes(Dalc)) + geom_bar()

```


```{r}
Dalc.model <- lm(G3 ~ Dalc, data = student_mat)
summary(Dalc.model)
```

Not significant


```{r}
ggplot(student_mat, aes(Walc)) + geom_bar()

```


```{r}

Walc.model <- lm(G3 ~ Walc, data = student_mat)
summary(Walc.model)
```


Not significant


## Using Multiple Regression

Upon further implementation, I realized there may be more than one explanatory variable for the output. To consider the variable in conjunction with the `G3` variable, I will be using multiple linear regression, and establishing my best naively implemented model to predict G3. 

vif to check for collinearity.


### Model Full

```{r}
model_full <- lm(G3 ~ romantic + freetime + goout + Dalc + Walc + famrel + activities, data= student_mat)
summary(model_full)
vif(model_full)
```


`Romantic` and `goout` variables are the only ones with p-values less than 0.05, indicating that they are statistically significant. 

### Drop Activities 
```{r}
model_full <- lm(G3 ~ romantic + freetime + goout + Dalc + Walc + famrel, data= student_mat)
summary(model_full)
vif(model_full)

```
### Drop Famrel

```{r}
model_full <- lm(G3 ~ romantic + freetime + goout + Dalc + Walc + activities, data= student_mat)
summary(model_full)
vif(model_full)
```

### Remove Walc

```{r}
model_full <- lm(G3 ~ romantic + freetime + goout + Dalc + famrel + activities, data= student_mat)
summary(model_full)
vif(model_full)
```



```{r}
model_full <- lm(G3 ~ romantic + freetime + goout + Walc + famrel + activities, data= student_mat)
summary(model_full)
vif(model_full)
```


```{r}
model_full <- lm(G3 ~ romantic + freetime + Dalc + Walc + famrel + activities, data= student_mat)
summary(model_full)
vif(model_full)
```


```{r}
model_full <- lm(G3 ~ romantic  + goout + Dalc + Walc + famrel + activities, data= student_mat)
summary(model_full)
vif(model_full)
```


```{r}
model_full <- lm(G3 ~ freetime + goout + Dalc + Walc + famrel + activities, data= student_mat)
summary(model_full)
vif(model_full)
```

Full Model:
Adjusted R-squared:  0.02282 

exclude activities:
Adjusted R-squared:  0.02501 <-- highest adjusted r sqaures

exclude famrel:
Adjusted R-squared:  0.02327 

exclude Wald:
Adjusted R-squared:  0.02465 

exclude Dalc:
Adjusted R-squared:  0.02439

exclude goout:
Adjusted R-squared:  0.006961 


exclude freetime:
Adjusted R-squared:  0.02328 

exclude romantic:

Adjusted R-squared:  0.009661 



Therefore we drop activities from the model


```{r}
model_full <- lm(G3 ~ romantic + freetime + goout + Dalc + Walc + activities, data= student_mat)
summary(model_full)
vif(model_full)
```
```{r}
model_full <- lm(G3 ~ romantic + freetime + goout + Dalc + Walc, data= student_mat)
summary(model_full)
vif(model_full)
```


```{r}
model_full <- lm(G3 ~ romantic + freetime + goout + Dalc , data= student_mat)
summary(model_full)
vif(model_full)
```
```{r}
model_full <- lm(G3 ~ romantic + freetime + goout , data= student_mat)
summary(model_full)
vif(model_full)
```
```{r}
model_full <- lm(G3 ~ romantic + goout , data= student_mat)
summary(model_full)
vif(model_full)
```

-----
```{r}
ideal.student.model <- lm(G3 ~ romantic + goout, data= student_mat)
summary(ideal.student.model)
vif(ideal.student.model)
```

When removing the other variables the most ideal variables are whether the student goes out or if they are in a romantic relationship. According to this model, as the grade increases, the romantic and goout variable will decrease by the given estimates. Indicating that the less the student does/has these things the higher their grade will be.

There might be some collinearity between the explanatory variables, given that when removed the estimates have changed. 


```{r}
ggplot(student_mat, aes(G3, goout, color=romantic)) +
  geom_col() +
  facet_wrap(vars(goout))
```



```{r}
one.way <- aov(G3 ~ romantic + goout, data = student_mat)
summary(one.way)
```


```{r}

ggplot(data = ideal.student.model, aes(x = .fitted, y = .resid)) +
  geom_point() +
  geom_jitter() +
  geom_hline(yintercept = 0, linetype = "dashed") +
  xlab("Fitted values") +
  ylab("Residuals")


```


```{r}
ggplot(ideal.student.model, aes(x=.resid)) +
  geom_histogram(binwidth = 1) +
  xlab("resid")
```

```{r}
ggplot(ideal.student.model, aes(sample = .resid)) +
 stat_qq() +
  xlab("resid")
```



By using the rlm model the error is lower than the lm.


```{r}
library(MASS)
robust <- rlm(G3 ~ romantic + goout, data=student_mat)
summary(robust)$sigma
summary(ideal.student.model)$sigma
```


---
## Removing the Outliers


```{r}
student.pass <- student_mat %>% filter(G3 != 0)
student.pass
```
### The New Full Model

```{r}
model_full <- lm(G3 ~ romantic + freetime + goout + Dalc + Walc + famrel + activities, data= student.pass)
summary(model_full)
vif(model_full)
```
### Linear Model

```{r}
model_full <- lm(G3 ~ goout , data= student.pass)
summary(model_full)

```

```{r}

ggplot(data = model_full, aes(x = .fitted, y = .resid)) +
  geom_point() +
  geom_jitter() +
  geom_hline(yintercept = 0, linetype = "dashed") +
  xlab("Fitted values") +
  ylab("Residuals")


```

```{r}
ggplot(model_full, aes(sample = .resid)) +
 stat_qq() +
  xlab("resid")
```

```{r}

ggplot(model_full, aes(x=.resid)) +
  geom_histogram(binwidth = .5) +
  xlab("resid")

```


https://book.stat420.org/transformations.html

---


```{r}
model_full <- lm(log(G3) ~ goout , data= student.pass)
summary(model_full)
#vif(model_full)
```


```{r}

ggplot(data = model_full, aes(x = .fitted, y = .resid)) +
  geom_point() +
  geom_jitter() +
  geom_hline(yintercept = 0, linetype = "dashed") +
  xlab("Fitted values") +
  ylab("Residuals")


```

```{r}

ggplot(model_full, aes(sample = .resid)) +
 stat_qq() +
  xlab("resid")

```

```{r}

ggplot(model_full, aes(x=.resid)) +
  geom_histogram(binwidth = .20) +
  xlab("resid")

```

By performing a logistic transformation of the G3 we get a much cleaner look at the residuals normal distribution.


```{r}
# Avg squared data change to the one before the log
sqrt(mean(resid(ideal.student.model) ^ 2))
```

```{r}
sqrt(mean(resid(model_full) ^ 2))
```

```{r}
sqrt(mean((student_mat$G3 - fitted(ideal.student.model)) ^ 2))

```

```{r}
student_mat.2 <- student_mat %>% filter(G3 != 0)
sqrt(mean((student_mat.2$G3 - exp(fitted(model_full))) ^ 2))
```

couldn't do logistic transformation do to some grades being 0 .

----

Logistic


## Change to a logistic model
```{r}
student.mat.binary <- student_mat %>%
      mutate(PassFail = ifelse(G3 >= 12,1,0))
student.mat.binary
```

```{r}
tab <- describeBy(student.mat.binary$goout, group = student.mat.binary$PassFail, mat = TRUE, skew = FALSE)
tab$group1 <- as.integer(as.character(tab$group1))
```


Dichoutimous outcome when we change it to pass fail



```{r}
lm.out <- lm(PassFail ~ goout, data = student.mat.binary)
summary(lm.out)
```


```{r}
student.mat.binary$linear_resid <- resid(lm.out)
ggplot(student.mat.binary, aes(x = goout, y = linear_resid)) +
    geom_point() +
    ggtitle('Residual Plot')
```





## Predicting Data

```{r}
train.rows <- sample(nrow(student.mat.binary), nrow(student.mat.binary) * .7)
student_train <- student.mat.binary[train.rows,]
student_test <- student.mat.binary[-train.rows,]
```

```{r}
student.pass <- table(student_train$PassFail) %>% prop.table
student.pass
```

```{r}
lr.out <- glm(PassFail ~ romantic + freetime + goout + Dalc + Walc + famrel + activities, data=student_train, family=binomial(link = 'logit'))
summary(lr.out)
```

```{r}
student_train
```



```{r}
student_train$prediction <- predict(lr.out, type = 'response', newdata = student_train)
ggplot(student_train, aes(x = prediction, color = PassFail == 1)) + geom_density()
```


```{r}
student_train$prediction_class <- student_train$prediction > 0.6
tab <- table(student_train$prediction_class, 
             student_train$PassFail) %>% prop.table() %>% print()
```




```{r}
new.student <- glm(PassFail ~ romantic + freetime + goout + Dalc + Walc + famrel + activities, data = student.mat.binary, family = binomial(link = "logit"))
summary(new.student)
```


```{r}
student.2 <- glm(PassFail ~ goout, data = student.mat.binary, family = binomial )
summary(student.2)
```

y = 0.36874 - 0.23757 * goout

gout ranges from 1 through 5 


The odds of passing

```{r}
logistic <- function(t) {
  return(1/ (1 + exp(-t)))
}

ggplot() + 
  stat_function(fun=logistic, n = 101) +
  xlim(-4,4) +
  xlab('t')
```



```{r}
library(chi)
1-pchisq(534.75,394)
```


```{r}
1-pchisq(528.24,393)
```


## Comparing the models

Compare the statistically significant models to the multiple

Given by the rsquared values this does not correctly 

# Conclusion
Why is this analysis important?
Limitations of the analysis?




```{r}
model_full3 <- lm(G3 ~ romantic + age + address+ famsize+ Pstatus + Medu + Fedu + Mjob + reason + guardian + traveltime + studytime + failures + schoolsup + paid + activities + nursery + higher + internet + freetime + health + absences + goout + Dalc + Walc + famrel + activities + sex, data= student_mat)
summary(model_full)
```


```{r}
ggplot(model_full3, aes(sample = .resid)) +
 stat_qq() +
  xlab("resid")
```

```{r}
ggplot(model_full3, aes(x=.resid)) +
  geom_histogram(binwidth = 1) +
  xlab("resid")

```

```{r}
ideal.student.model.3 <- lm(G3 ~ romantic  + failures   , data= student_mat)
summary(ideal.student.model.3)
```

```{r}
ggplot(ideal.student.model.3, aes(x=.resid)) +
  geom_histogram(binwidth = 1) +
  xlab("resid")

```

https://statisticsbyjim.com/regression/multicollinearity-in-regression-analysis/#:~:text=The%20potential%20solutions%20include%20the,or%20partial%20least%20squares%20regression.


https://www.statology.org/robust-regression-in-r/

