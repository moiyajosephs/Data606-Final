---
title: "Josephs-Final"
author: "Moiya Josephs"
date: "2022-10-18"
output: pdf_document
---

## Data Preparation

```{r setup, echo=TRUE, results='hide', warning=FALSE, message=FALSE}
# load data
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(ggplot2)
library(curl)
library(psych)
library(car)
```

## Abstract


## Research question 

Does having a social life predict final grade? 


## Cases 

**What are the cases, and how many are there?**

There are 649 cases that represent students and their achievements in secondary education in two Portuguese schools.

## Data collection 

**Describe the method of data collection.**

The data was collected using school reports and questionnaires.

## Type of study 

**What type of study is this (observational/experiment)?**

This is observational study.

## Data Source 

**If you collected the data, state self-collected. If not, provide a citation/link.**

The data was collected by University of Minho and the dataset can be found here: 
Source: https://archive.ics.uci.edu/ml/datasets/Student+Performance


### Dependent Variable

**What is the response variable? Is it quantitative or qualitative?**

The response variable is the final grade of each student. It is quantitative.

### Independent Variable(s)

The independent variable is the variables that describe the students social life. To answer this questions the variables I classify as describing a students social life is their activities, romantic, family relationship, free time, going out and alcohol consumption. They are all qualitative.



## Relevant summary statistics 


```{r message=FALSE}
student_mat_csv <- "https://raw.githubusercontent.com/moiyajosephs/Data606-Final/main/student-mat.csv"
student_mat <- read_delim(curl(student_mat_csv),delim = ";")
```


```{r}
summary(student_mat)
```


```{r}
ggplot(student_mat, aes(sex)) + geom_bar()
```

```{r}
ggplot(student_mat, aes(G3)) + geom_bar() + facet_wrap(vars(sex))
```

Normally distributed with one obvious outlier. 



## The Romance Model

## Romantic

Students reply either yes or no if they are in a romantic relationship, indicated by the `romantic` variable.

```{r}

ggplot(student_mat, aes(romantic)) + geom_bar()
```


```{r}
romantic.model <- lm(G3 ~ romantic, data = student_mat)
summary(romantic.model)
```

```{r}
ggplot(student_mat, aes(x= G3)) +
  geom_histogram(bins = 10) +
  facet_wrap(vars(romantic))
```


Romantic is statistically significant according to the anova test. 

```{r}
summary_stats <- student_mat %>%
  group_by(romantic) %>%
  summarise(mean_by_group = mean(G3))
summary_stats
```


```{r}
describeBy(student_mat$G3, 
           group = student_mat$romantic, mat=TRUE)
```

```{r}
one.way <- aov(G3 ~ romantic, data = student_mat)
summary(one.way)
```

### Analyzing the r^2


```{r}
ggplot(data = romantic.model, aes(x = .fitted, y = .resid)) +
  geom_point() + 
  geom_jitter()+
  geom_hline(yintercept = 0, linetype = "dashed") +
  xlab("Fitted values") +
  ylab("Residuals")

```



## The Activities Model
Students reply yes or no if they have extra curricular activities, indicated by the `activities` variables.


```{r}
ggplot(student_mat, aes(x= G3)) +
  geom_histogram(bins = 10) +
  facet_wrap(vars(activities))
```


```{r}
activities.model <- lm(G3 ~ activities, data = student_mat)
summary(activities.model)
```
By looking at the summary of the `activities.model` we can see that activites estimate is not statistically significant as indicated by p-value which is greater than 0.05. As well as the F-statistic and p value given, which is also greater than 0.05.


```{r}

ggplot(data = activities.model, aes(x = .fitted, y = .resid)) +
  geom_point() +
  geom_jitter() +
  geom_hline(yintercept = 0, linetype = "dashed") +
  xlab("Fitted values") +
  ylab("Residuals")


```



## Freetime 

Freetime after school rated from 1 very low to 5 very high.

```{r}
ggplot(student_mat, aes(freetime)) + geom_bar()

```
```{r}
ggplot(student_mat, aes(x= G3)) +
  geom_histogram(bins = 10) +
  facet_wrap(vars(activities))
```



```{r}
freetime.model <- lm(G3 ~ freetime, data = student_mat)
summary(freetime.model)
```
Not significant!



## Family Relationship

`famrel` indicated the quality of family relationships on a scale from (numeric: from 1 - very bad to 5 - excellent)


```{r}
ggplot(student_mat, aes(famrel)) + geom_bar()

```

```{r}
famrel.model <- lm(G3 ~ famrel, data = student_mat)
summary(famrel.model)
```

No significance

## Going Out

`goout` indicates if students go out with friends rated from (numeric: from 1 - very low to 5 - very high)


```{r}
ggplot(student_mat, aes(goout)) + geom_bar()

```


```{r}
goout.model <- lm(G3 ~ goout, data = student_mat)
summary(goout.model)

```

Significance!!





## Alcohol Consumption

Dalc indicated the workday alcohol consumption from very low to very high and Walc indicates weekend alcohol consumption, also from very low to very high.


```{r}
ggplot(student_mat, aes(Dalc)) + geom_bar()

```


```{r}
Dalc.model <- lm(G3 ~ Dalc, data = student_mat)
summary(Dalc.model)
```

Not significant


```{r}
ggplot(student_mat, aes(Walc)) + geom_bar()

```


```{r}

Walc.model <- lm(G3 ~ Walc, data = student_mat)
summary(Walc.model)
```


Not significant


## Using Multiple Regression

Upon further implementation, I realized there may be more than one explanatory variable for the output. To consider the variable in conjunction with the `G3` variable, I will be using multiple linear regression, and establishing my best naively implemented model to predict G3. 

vif to check for collinearity.


### Model Full

```{r}
model_full <- lm(G3 ~ romantic + freetime + goout + Dalc + Walc + famrel + activities, data= student_mat)
summary(model_full)
vif(model_full)
```


`Romantic` and `goout` variables are the only ones with p-values less than 0.05, indicating that they are statistically significant. 


## Linear Regression for full model of social activitiess



```{r}
model.full <- lm(G3 ~ romantic + freetime + goout + Walc + famrel + activities, data= student_mat)
summary(model.full)
vif(model.full)
```



```{r}
model.reduced <- lm(G3 ~ romantic + goout , data= student_mat)
summary(model.reduced)
vif(model.reduced)
```


When removing the other variables the most ideal variables are whether the student goes out or if they are in a romantic relationship. According to this model, as the grade increases, the romantic and goout variable will decrease by the given estimates. Indicating that the less the student does/has these things the higher their grade will be.




```{r}
ggplot(student_mat, aes(G3, goout, color=romantic)) +
  geom_col() +
  facet_wrap(vars(goout))
```



```{r}

ggplot(data = model.reduced, aes(x = .fitted, y = .resid)) +
  geom_point() +
  geom_jitter() +
  geom_hline(yintercept = 0, linetype = "dashed") +
  xlab("Fitted values") +
  ylab("Residuals")


```


```{r}
ggplot(model.reduced, aes(x=.resid)) +
  geom_histogram(binwidth = 1) +
  xlab("resid")
```

```{r}
ggplot(model.reduced, aes(sample = .resid)) +
    stat_qq_line() +
 stat_qq() +
  xlab("resid")
```



By using the rlm model the error is lower than the lm.


```{r}
library(MASS)
robust <- rlm(G3 ~ romantic + goout, data=student_mat)
summary(robust)
```
```{r}
ggplot(robust, aes(sample = .resid)) +
    stat_qq_line() +
 stat_qq() +
  xlab("resid")
```




```{r}
summary(robust)$sigma
summary(ideal.student.model)$sigma
```


---
## Removing the Outliers


```{r}
student.pass <- student_mat %>% filter(G3 != 0)
student.pass
```
### The New Full Model

```{r}
model_full <- lm(G3 ~ romantic + freetime + goout + Dalc + Walc + famrel + activities, data= student.pass)
summary(model_full)
vif(model_full)
```
### Linear Model


```{r}
model_full <- lm(G3 ~ goout , data= student.pass)
summary(model_full)

```

```{r}

ggplot(data = model_full, aes(x = .fitted, y = .resid)) +
  geom_point() +
  geom_jitter() +
  geom_hline(yintercept = 0, linetype = "dashed") +
  xlab("Fitted values") +
  ylab("Residuals")


```

```{r}
ggplot(model_full, aes(sample = .resid)) +
 stat_qq() +
    stat_qq_line() +
  xlab("resid")
```

```{r}

ggplot(model_full, aes(x=.resid)) +
  geom_histogram(binwidth = .5) +
  xlab("resid")

```


https://book.stat420.org/transformations.html

---


```{r}
model_full <- lm(log(G3) ~ goout , data= student.pass)
summary(model_full)
#vif(model_full)
```


```{r}

ggplot(data = model_full, aes(x = .fitted, y = .resid)) +
  geom_point() +
  geom_jitter() +
  geom_hline(yintercept = 0, linetype = "dashed") +
  xlab("Fitted values") +
  ylab("Residuals")


```

```{r}

ggplot(model_full, aes(sample = .resid)) +
 stat_qq() +
  stat_qq_line() +
  xlab("resid")

```

```{r}

ggplot(model_full, aes(x=.resid)) +
  geom_histogram(binwidth = .25) +
  xlab("resid")

```

By performing a logistic transformation of the G3 we get a much cleaner look at the residuals normal distribution.


```{r}
# Avg squared data change to the one before the log
sqrt(mean(resid(ideal.student.model) ^ 2))
```

```{r}
sqrt(mean(resid(model_full) ^ 2))
```

```{r}
sqrt(mean((student_mat$G3 - fitted(ideal.student.model)) ^ 2))

```

```{r}
student_mat.2 <- student_mat %>% filter(G3 != 0)
sqrt(mean((student_mat.2$G3 - exp(fitted(model_full))) ^ 2))
```

couldn't do logistic transformation do to some grades being 0 .

----

Logistic


## Change to a logistic model
```{r}
student.mat.binary <- student_mat %>%
      mutate(PassFail = ifelse(G3 >= 12,1,0))
student.mat.binary
```

```{r}
tab <- describeBy(student.mat.binary$goout, group = student.mat.binary$PassFail, mat = TRUE, skew = FALSE)
tab$group1 <- as.integer(as.character(tab$group1))
```


Dichotomous outcome when we change it to pass fail


```{r}
student.mat.binary.full <- glm(PassFail ~ romantic + freetime + goout + Dalc + Walc + famrel + activities, data = student.mat.binary, family = binomial(link = "logit"))
summary(student.mat.binary.full)
```




```{r}
student.mat.binary.reduced <- glm(PassFail ~  goout, data = student.mat.binary, family = binomial(link = "logit"))
summary(student.mat.binary.reduced)
```
---

```{r}
set.seed(811)
train.rows <- sample(nrow(student.mat.binary), nrow(student.mat.binary) * .7)
student_train <- student.mat.binary[train.rows,]
student_test <- student.mat.binary[-train.rows,]
```


```{r}
student_fail <- table(student_train$PassFail) %>% prop.table
student_fail
```




```{r}
student.model.training <- glm(PassFail ~ romantic + freetime + goout + Dalc + Walc + famrel + activities, data = student_train, family = binomial(link = "logit"))
summary(student.model.training)

```



```{r}
student_train$prediction <- predict(student.model.training, type = 'response', newdata = student_train)
ggplot(student_train, aes(x = prediction, color = PassFail == 1)) + geom_density()
```


```{r}
student_train$prediction_class <- student_train$prediction > 0.5
tab <- table(student_train$prediction_class, 
             student_train$PassFail) %>% prop.table() %>% print()
```


```{r}

accuracy <- sum(tab[1], tab[4]) / sum(tab[1:4])
precision <- tab[4] / sum(tab[4], tab[2])
sensitivity <- tab[4] / sum(tab[4], tab[3])
fscore <- (2 * (sensitivity * precision))/(sensitivity + precision)
specificity <- tab[1] / sum(tab[1], tab[2])
```


```{r}
accuracy
```
The overall accuracy of this model is 63.40%. Considering that 58.69565 % of the data set population has failed their final exam and would be used for guessing. 


```{r}
#The true positive rate: the percentage of individuals the model correctly predicted would pass 

sensitivity

```


```{r}

#the true negative: percentage the model correctly predicted would fail
specificity
```


```{r}
library(pROC)
roc.info <- roc(student_train$PassFail, student.model.training$fitted.values, plot = TRUE, print.auc = TRUE)
```

```{r}

roc.df <- data.frame(
  tpp=roc.info$sensitivities*100,
  fpp=(1 - roc.info$specificities) * 100,
  thresholds=roc.info$thresholds
)

head(roc.df)

```


```{r}
tail(roc.df)
```


A poor classifier (like Classifier 2 below) will not able to distinguish the two classes well and therefore its ROC curve will be closer to the diagonal. Implying lower TPR rates (more false negatives) and higher FPR rates (more false positives). This leads us to the concept of AUC or Area Under the Curve discussed in point c.

## For passing



```{r}

student_pass <- table(student_test$PassFail) %>% prop.table()
student_pass

```


```{r}
student_test$prediction <- predict(student.model.training, newdata = student_test, type = 'response')
student_test$prediciton_class <- student_test$prediction > 0.5
tab_test <- table(student_test$prediciton_class, student_test$PassFail) %>%
prop.table() %>% print()
```


```{r}

accuracy <- sum(tab_test[1], tab_test[4]) / sum(tab_test[1:4])
precision <- tab_test[4] / sum(tab_test[4], tab_test[2])
sensitivity <- tab_test[4] / sum(tab_test[4], tab_test[3])
fscore <- (2 * (sensitivity * precision))/(sensitivity + precision)
specificity <- tab_test[1] / sum(tab_test[1], tab_test[2])
```

```{r}
accuracy
```

41% passed their final exam

```{r}
sensitivity
```


```{r}
specificity
```



https://www.technologynetworks.com/analysis/articles/sensitivity-vs-specificity-318222








## Comparing the models

Compare the statistically significant models to the multiple

Given by the rsquared values this does not correctly 

# Conclusion
Why is this analysis important?
Limitations of the analysis?


Conclusion on how to tune the analysis

The model would do better with all the variables





```{r}
model_full3 <- glm(PassFail ~ romantic + age + address+ famsize+ Pstatus + Medu + Fedu + Mjob + reason + guardian + traveltime + studytime + failures + schoolsup + paid + activities + nursery + higher + internet + freetime + health + absences + goout + Dalc + Walc + famrel + activities + sex + G1 + G2, data= student.mat.binary)
summary(model_full3)
vif(model_full3)
```


```{r}
ggplot(model_full3, aes(sample = .resid)) +
 stat_qq() +
  stat_qq_line() +
  xlab("resid")
```

```{r}
ggplot(model_full3, aes(x=.resid)) +
  geom_histogram(binwidth = .1) +
  xlab("resid")

```

```{r}
ideal.student.model.3 <- glm(PassFail ~ absences  + G1  + G2  , data= student.mat.binary)
summary(ideal.student.model.3)
```

```{r}
ggplot(ideal.student.model.3, aes(x=.resid)) +
  geom_histogram(binwidth = .1) +
  xlab("resid")

```


```{r}
ggplot(ideal.student.model.3, aes(sample = .resid)) +
 stat_qq() +
  xlab("resid")
```





https://statisticsbyjim.com/regression/multicollinearity-in-regression-analysis/#:~:text=The%20potential%20solutions%20include%20the,or%20partial%20least%20squares%20regression.


https://www.statology.org/robust-regression-in-r/

