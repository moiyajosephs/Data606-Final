---
title: "Josephs-Final"
author: "Moiya Josephs"
date: "2022-10-18"
output: pdf_document
---

## Data Preparation

```{r setup, echo=TRUE, results='hide', warning=FALSE, message=FALSE}
# load data
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(ggplot2)
library(curl)
library(psych)
library(car)
```

## Abstract


## Research question 

Does having a social life predict final grade? 


## Cases 

**What are the cases, and how many are there?**

There are 649 cases that represent students and their achievements in secondary education in two Portuguese schools.

## Data collection 

**Describe the method of data collection.**

The data was collected using school reports and questionnaires.

## Type of study 

**What type of study is this (observational/experiment)?**

This is observational study.

## Data Source 

**If you collected the data, state self-collected. If not, provide a citation/link.**

The data was collected by University of Minho and the dataset can be found here: 
Source: https://archive.ics.uci.edu/ml/datasets/Student+Performance


### Dependent Variable

**What is the response variable? Is it quantitative or qualitative?**

The response variable is the final grade of each student. It is quantitative.

### Independent Variable(s)

The independent variable is the variables that describe the students social life. To answer this questions the variables I classify as describing a students social life is their activities, romantic, family relationship, free time, going out and alcohol consumption. They are all qualitative.



## Relevant summary statistics 


```{r message=FALSE}
student_mat_csv <- "https://raw.githubusercontent.com/moiyajosephs/Data606-Final/main/student-mat.csv"
student_mat <- read_delim(curl(student_mat_csv),delim = ";")
```


```{r}
summary(student_mat)
```
 Attributes for both student-mat.csv (Math course):
1 school - student's school (binary: 'GP' - Gabriel Pereira or 'MS' - Mousinho da Silveira)
2 sex - student's sex (binary: 'F' - female or 'M' - male)
3 age - student's age (numeric: from 15 to 22)
4 address - student's home address type (binary: 'U' - urban or 'R' - rural)
5 famsize - family size (binary: 'LE3' - less or equal to 3 or 'GT3' - greater than 3)
6 Pstatus - parent's cohabitation status (binary: 'T' - living together or 'A' - apart)
7 Medu - mother's education (numeric: 0 - none, 1 - primary education (4th grade), 2 â€“ 5th to 9th grade, 3 â€“ secondary education or 4 â€“ higher education)
8 Fedu - father's education (numeric: 0 - none, 1 - primary education (4th grade), 2 â€“ 5th to 9th grade, 3 â€“ secondary education or 4 â€“ higher education)
9 Mjob - mother's job (nominal: 'teacher', 'health' care related, civil 'services' (e.g. administrative or police), 'at_home' or 'other')
10 Fjob - father's job (nominal: 'teacher', 'health' care related, civil 'services' (e.g. administrative or police), 'at_home' or 'other')
11 reason - reason to choose this school (nominal: close to 'home', school 'reputation', 'course' preference or 'other')
12 guardian - student's guardian (nominal: 'mother', 'father' or 'other')
13 traveltime - home to school travel time (numeric: 1 - <15 min., 2 - 15 to 30 min., 3 - 30 min. to 1 hour, or 4 - >1 hour)
14 studytime - weekly study time (numeric: 1 - <2 hours, 2 - 2 to 5 hours, 3 - 5 to 10 hours, or 4 - >10 hours)
15 failures - number of past class failures (numeric: n if 1<=n<3, else 4)
16 schoolsup - extra educational support (binary: yes or no)
17 famsup - family educational support (binary: yes or no)
18 paid - extra paid classes within the course subject (Math or Portuguese) (binary: yes or no)
19 activities - extra-curricular activities (binary: yes or no)
20 nursery - attended nursery school (binary: yes or no)
21 higher - wants to take higher education (binary: yes or no)
22 internet - Internet access at home (binary: yes or no)
23 romantic - with a romantic relationship (binary: yes or no)
24 famrel - quality of family relationships (numeric: from 1 - very bad to 5 - excellent)
25 freetime - free time after school (numeric: from 1 - very low to 5 - very high)
26 goout - going out with friends (numeric: from 1 - very low to 5 - very high)
27 Dalc - workday alcohol consumption (numeric: from 1 - very low to 5 - very high)
28 Walc - weekend alcohol consumption (numeric: from 1 - very low to 5 - very high)
29 health - current health status (numeric: from 1 - very bad to 5 - very good)
30 absences - number of school absences (numeric: from 0 to 93)

# these grades are related with the course subject, Math or Portuguese:
31 G1 - first period grade (numeric: from 0 to 20)
31 G2 - second period grade (numeric: from 0 to 20)
32 G3 - final grade (numeric: from 0 to 20, output target)



```{r}
ggplot(student_mat, aes(sex)) + geom_bar()
```

```{r}
ggplot(student_mat, aes(G3)) + geom_bar() + facet_wrap(vars(sex))
```



```{r}
ggplot(student_mat, aes(G3)) + geom_bar() + facet_grid(vars(school), vars(sex))
```



## The Romance Model

## Romantic

Students reply either yes or no if they are in a romantic relationship, indicated by the `romantic` variable.

```{r}

ggplot(student_mat, aes(romantic)) + geom_bar()
```


```{r}
romantic.model <- lm(G3 ~ romantic + sex + age, data = student_mat)
summary(romantic.model)
```

```{r}
ggplot(student_mat, aes(x= G3)) +
  geom_histogram(bins = 10) +
  facet_grid(vars(romantic), vars(sex))
```


Romantic is statistically significant according to the anova test. 

```{r}
summary_stats <- student_mat %>%
  group_by(romantic) %>%
  summarise(mean_by_group = mean(G3))
summary_stats
```


```{r}
describeBy(student_mat$G3, 
           group = student_mat$romantic, mat=TRUE)
```

```{r}
one.way <- aov(G3 ~ romantic + sex + age, data = student_mat)
summary(one.way)
```

### Analyzing the r^2


```{r}
ggplot(data = romantic.model, aes(x = .fitted, y = .resid)) +
  geom_point() + 
  geom_jitter()+
  geom_hline(yintercept = 0, linetype = "dashed") +
  xlab("Fitted values") +
  ylab("Residuals")

```



## The Activities Model
Students reply yes or no if they have extra curricular activities, indicated by the `activities` variables.


```{r}
ggplot(student_mat, aes(x= G3)) +
  geom_histogram(bins = 10) +
  facet_wrap(vars(activities))
```


```{r}
activities.model <- lm(G3 ~ activities + sex + age, data = student_mat)
summary(activities.model)
```
By looking at the summary of the `activities.model` we can see that activites estimate is not statistically significant as indicated by p-value which is greater than 0.05. As well as the F-statistic and p value given, which is also greater than 0.05.


```{r}

ggplot(data = activities.model, aes(x = .fitted, y = .resid)) +
  geom_point() +
  geom_jitter() +
  geom_hline(yintercept = 0, linetype = "dashed") +
  xlab("Fitted values") +
  ylab("Residuals")


```



## Freetime 

Freetime after school rated from 1 very low to 5 very high.

```{r}
ggplot(student_mat, aes(freetime)) + geom_bar()

```
```{r}
ggplot(student_mat, aes(x= G3)) +
  geom_histogram(bins = 10) +
  facet_wrap(vars(activities))
```



```{r}
freetime.model <- lm(G3 ~ freetime + sex + age, data = student_mat)
summary(freetime.model)
```
Not significant!



## Family Relationship

`famrel` indicated the quality of family relationships on a scale from (numeric: from 1 - very bad to 5 - excellent)


```{r}
ggplot(student_mat, aes(famrel)) + geom_bar()

```

```{r}
famrel.model <- lm(G3 ~ famrel+ sex + age, data = student_mat)
summary(famrel.model)
```

No significance

## Going Out

`goout` indicates if students go out with friends rated from (numeric: from 1 - very low to 5 - very high)


```{r}
ggplot(student_mat, aes(goout)) + geom_bar()

```


```{r}
goout.model <- lm(G3 ~ goout+ sex + age, data = student_mat)
summary(goout.model)

```

Significance!!




## Using Multiple Regression

Upon further implementation, I realized there may be more than one explanatory variable for the output. To consider the variable in conjunction with the `G3` variable, I will be using multiple linear regression, and establishing my best naively implemented model to predict G3. 

When using many variables in a linear regression, there may also exist a collinearity between them. To determine if there are any such vairables I used `vif` to check for collinearity.








## Linear Regression for full model of social activitiess


```{r}
model.full <- lm(G3 ~ romantic + freetime + goout + Walc + famrel + activities+ sex + age + Dalc, data= student_mat)
summary(model.full)
vif(model.full)
```

`Romantic` and `goout` variables are the only ones with p-values less than 0.05, indicating that they are statistically significant.


Get rid of activitiesd:

Walc:


freetime:



+ Dalc 0.05082 



```{r}
model.reduced <- lm(G3 ~ romantic + goout  + famrel + sex + age, data= student_mat)
summary(model.full)
vif(model.full)
```




Model full linear model equation is:

y =  18.6197 -0.9137 * romanticyes -0.5201 * goout + 0.2802 * famrel + 0.8842 * sexM - 0.4675 * age

The model has an adjusted R squared of 0.0508. 


By reducing the model and calling it `model.reduced` we can now closely evaluate if the two variables are a better model fit. 


```{r}
model.reduced <- lm(G3 ~ goout + sex + age, data= student_mat)
summary(model.reduced)
vif(model.reduced)
```


When removing the other variables the most ideal variables are whether the student goes out or if they are in a romantic relationship. According to this model, as the grade increases, the `romanticyes` and `goout` variable will decrease by the given estimates. Indicating that the less the student does/has these things the higher their grade will be.


| Model           | Adjusted R  |
| --------------- | ----------- |
| model.full      | 0.04234     |
| model.reduced   | 0.0508      |


Model reduced has a better adjusted R but the increase is not by a significant amount. 

```{r}

ggplot(data = model.reduced, aes(x = .fitted, y = .resid)) +
  geom_point() +
  geom_jitter() +
  geom_hline(yintercept = 0, linetype = "dashed") +
  xlab("Fitted values") +
  ylab("Residuals")


```


```{r}
ggplot(model.reduced, aes(x=.resid)) +
  geom_histogram(binwidth = 1) +
  xlab("resid")
```



```{r}
ggplot(model.reduced, aes(sample = .resid)) +
    stat_qq_line() +
 stat_qq() +
  xlab("resid")
```
Plotting the residuals show that the residuals are not normally distributed. Likely due to the outliers of 0 values in the data as well.


### Using robust linear regression

In trying to find the best model to fit this data, I researched models that perform better than lm when there are outliers. In my search I discovered the `rlm` function from the library MASS. 

By using the rlm model the error is lower than the lm.


#### Robust Full

```{r}
library(MASS)
```



```{r}
robust.model.full <- rlm(G3 ~ romantic + freetime + goout + Walc + famrel + activities + sex + age, data= student_mat)
summary(robust.model.full)
vif(robust.model.full)
```



```{r}
robust.model.reduced <- rlm(G3 ~ goout + sex + age, data=student_mat)
summary(robust.model.reduced)
vif(model.full)
```


---
## Removing the Outliers

Next I tried to remove the outliers to see if they were having a significant affect on the models. 



```{r}
student.pass <- student_mat %>% filter(G3 != 0)
student.pass
```
### The New Full Model

```{r}
model.full <- lm(G3 ~ romantic + freetime + goout + famrel + activities + sex + age +Walc + Dalc, data= student.pass)
summary(model.full)
vif(model.full)
```
### Linear Model

Now without the outliers only `sex` is the only statistically significant variable.

Using backwards elimination I cut freetime


+ romantic:


famrel

activitied: 0.06805 

+ dalc: 06947 


And arrive at the final model below




```{r}
model.reduced <- lm(G3 ~  goout + age  + sex + Walc, data= student.pass)
summary(model.reduced)
vif(model.reduced)
```




```{r}

ggplot(data = model.reduced, aes(x = .fitted, y = .resid)) +
  geom_point() +
  geom_jitter() +
  geom_hline(yintercept = 0, linetype = "dashed") +
  xlab("Fitted values") +
  ylab("Residuals")


```

```{r}
ggplot(model.reduced, aes(sample = .resid)) +
 stat_qq() +
    stat_qq_line() +
  xlab("resid")
```

```{r}

ggplot(model.reduced, aes(x=.resid)) +
  geom_histogram(binwidth = 1) +
  xlab("resid")

```


https://book.stat420.org/transformations.html


----

Logistic


## Change to a logistic model
```{r}
student.mat.binary <- student_mat %>%
      mutate(PassFail = ifelse(G3 >= 12,1,0))
student.mat.binary
```

```{r}
tab <- describeBy(student.mat.binary$goout, group = student.mat.binary$PassFail, mat = TRUE, skew = FALSE)
tab$group1 <- as.integer(as.character(tab$group1))
```




```{r}
set.seed(811)
train.rows <- sample(nrow(student.mat.binary), nrow(student.mat.binary) * .7)
student_train <- student.mat.binary[train.rows,]
student_test <- student.mat.binary[-train.rows,]
```


```{r}
student_fail <- table(student_train$PassFail) %>% prop.table
student_fail
```




```{r}
student.model.training <- glm(PassFail ~ romantic + freetime + goout + Dalc + Walc + famrel + activities + sex + age, data = student_train, family = binomial(link = "logit"))
summary(student.model.training)

```



```{r}
student_train$prediction <- predict(student.model.training, type = 'response', newdata = student_train)
ggplot(student_train, aes(x = prediction, color = PassFail == 1)) + geom_density()
```


```{r}
student_train$prediction_class <- student_train$prediction > 0.5
tab <- table(student_train$prediction_class, 
             student_train$PassFail) %>% prop.table() %>% print()
```


```{r}

accuracy <- sum(tab[1], tab[4]) / sum(tab[1:4])
precision <- tab[4] / sum(tab[4], tab[2])
sensitivity <- tab[4] / sum(tab[4], tab[3])
fscore <- (2 * (sensitivity * precision))/(sensitivity + precision)
specificity <- tab[1] / sum(tab[1], tab[2])
```


```{r}
accuracy
```
The overall accuracy of this model is 63.40%. Considering that 58.69565 % of the data set population has failed their final exam and would be used for guessing. 


```{r}
#The true positive rate: the percentage of individuals the model correctly predicted would pass 

sensitivity

```


```{r}

#the true negative: percentage the model correctly predicted would fail
specificity
```


```{r}
library(pROC)
roc.info <- roc(student_train$PassFail, student.model.training$fitted.values, plot = TRUE, print.auc = TRUE)
```

```{r}

roc.df <- data.frame(
  tpp=roc.info$sensitivities*100,
  fpp=(1 - roc.info$specificities) * 100,
  thresholds=roc.info$thresholds
)

head(roc.df)

```


```{r}
tail(roc.df)
```


A poor classifier (like Classifier 2 below) will not able to distinguish the two classes well and therefore its ROC curve will be closer to the diagonal. Implying lower TPR rates (more false negatives) and higher FPR rates (more false positives). This leads us to the concept of AUC or Area Under the Curve discussed in point c.

## For passing



```{r}

student_pass <- table(student_test$PassFail) %>% prop.table()
student_pass

```


```{r}
student_test$prediction <- predict(student.model.training, newdata = student_test, type = 'response')
student_test$prediciton_class <- student_test$prediction > 0.5
tab_test <- table(student_test$prediciton_class, student_test$PassFail) %>%
prop.table() %>% print()
```


```{r}

accuracy <- sum(tab_test[1], tab_test[4]) / sum(tab_test[1:4])
precision <- tab_test[4] / sum(tab_test[4], tab_test[2])
sensitivity <- tab_test[4] / sum(tab_test[4], tab_test[3])
fscore <- (2 * (sensitivity * precision))/(sensitivity + precision)
specificity <- tab_test[1] / sum(tab_test[1], tab_test[2])
```

```{r}
accuracy
```

41% passed their final exam

```{r}
sensitivity
```


```{r}
specificity
```








## Comparing the models

Compare the statistically significant models to the multiple

Given by the rsquared values this does not correctly 

# Conclusion
Why is this analysis important?
This analysis shows that there is some relationship between the social lives of these students in the study and to their final grades. These models however, could be better tuned with the other variables collected in the study. 

Limitations of this analysis are that it is one sample size of only two schools in one country. There also could have been missing data, human error that can account for a lot of the values (the abundant 0s). Having that outlier can skew the model and not be a clear indicator. Another consideration to make is there are many other variables that were not included that could have helped with the model tuning. 


The linear regression model performed better than I expected. After tuning the model to increase the R adjusted I 






https://www.technologynetworks.com/analysis/articles/sensitivity-vs-specificity-318222

https://towardsdatascience.com/demystifying-roc-curves-df809474529a

https://statisticsbyjim.com/regression/multicollinearity-in-regression-analysis/#:~:text=The%20potential%20solutions%20include%20the,or%20partial%20least%20squares%20regression.


https://www.statology.org/robust-regression-in-r/

